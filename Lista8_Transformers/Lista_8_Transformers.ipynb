{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wv0PpBLtFnC"
      },
      "source": [
        "# Lista 8 - Transformers\n",
        "\n",
        "Na lista passada, exploramos mecanismos de atenção como métodos adicionais a serem aplicados a redes recorrentes para melhorar seus resultados. Nessa lista exploraremos o modelo Transformer, a primeira arquitetura a incorporar o mecanismo de atenção como parte central do processamento.\n",
        "Nesta lista nos basearemos [neste artigo ilustrado](http://jalammar.github.io/illustrated-transformer/) para implementar uma versão simplificada da arquitetura do Transformer. A ideia é que implementar essa versão simplificada nos ajudará a entender quais peças a constituem e como cada um delas se encaixa.\n",
        "\n",
        "Essa definitivamente não é maneira como um Transformer é de fato implementado, porque questões de complexidade e eficiência são bastante relevantes, mas ao retirá-las do caminho, poderemos entender melhor a estrutura conceitual e algoritmica do modelo.\n",
        "\n",
        "De forma supercial, um tranformer é constituído de duas partes, uma componente de codificação e uma componente de decodificação. A componente de codificação é composta por uma pilha de Encoders e a componente de decofdificação por uma pilha de Decoders. Nesta lista implementaremos uma versão simplificada de um Encoder.\n",
        "\n",
        "**Dica: Quando necessário use as funções matmul e softmax importadas abaixo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dMdIFnA11PJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import matmul\n",
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L3tV3SHvpe6"
      },
      "source": [
        "# <font color='blue'>  Questão 1 </font>\n",
        "\n",
        "Comece implementado o mecanismo de auto-atenção, seguindo a maneira como ele é descrito [no artigo](http://jalammar.github.io/illustrated-transformer/). Use apenas a biblioteca numpy, não use o tensorflow, keras ou semelhantes. Essa não será uma versão eficiente, mas deve ser fácil de ser lida e entendida. Não se preocupe com o treinamento. Apenas considere que são fornecidas a matriz de input e as matrizes de query, key  e value já calculados e implemente uma função que aplique sobre elas o mecanismo de auto-atenção e retorne uma nova matriz de representações, representada pela variável output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbbwjuvS12rC"
      },
      "outputs": [],
      "source": [
        "def self_attention(X, query, key, value):\n",
        "    #Sua resposta começa aqui\n",
        "    \n",
        "    #Sua resposta acaba aqui\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYP30JpCw1QN"
      },
      "source": [
        "Com o mecanismo de auto-atenção implementado, podemos agora implementar a camada de encoder em si. Para te ajudar, já fornecemos algumas peças: uma rede feed-foward simplificada e uma camada de normalização. Basta encaixá-las nos lugares corretos dentro da camada e garantir que as partes por você implementadas estejam corretas e compatíveis com essas funções. Não modifique as funções feed_foward e layer_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCepmnIG2gOQ"
      },
      "outputs": [],
      "source": [
        "def feed_forward(x):\n",
        "    embedding_length = x.shape[-1]\n",
        "    inner_dim = 4 * embedding_length\n",
        "    ff_result = matmul(x, np.random.randn(embedding_length, inner_dim))\n",
        "    ff_result = np.maximum(0, ff_result)  # ReLU activation\n",
        "    ff_result = matmul(ff_result, np.random.randn(inner_dim, embedding_length))\n",
        "    return ff_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWdN36ex2wQu"
      },
      "outputs": [],
      "source": [
        "def layer_norm(x):\n",
        "    norm_x = (x - np.mean(x, axis=-1, keepdims=True)) / np.sqrt(np.var(x, axis=-1, keepdims=True) + 1e-8)\n",
        "    return norm_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgqwp8cTxXAH"
      },
      "source": [
        "# <font color='blue'>  Questão 2 </font>\n",
        "\n",
        "Agora que você tem as peças necessárias, implemente na função abaixo a arquitetura de uma camada de Encoder de transformer. Novamente, siga a arquitetura como definida [neste artigo](http://jalammar.github.io/illustrated-transformer/), use apenas o numpy e não se precupe com o treinamento dos parâmetros, suponha apenas que a entrada x é dada e que as matrizes Wq, Wk, Wv e Wo já foram treinadas.\n",
        "\n",
        "Para simplificar, implemente o encoder como tendo **uma única cabeça de atenção (attention-head)**. Note que a matriz Wo ainda é necessária mesmo quando existe apenas uma camada de atenção, pois ela serve como uma forma de converter a dimensionalidade da matriz retornada pela camada de auto-atenção."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itDXW3NV28VY"
      },
      "outputs": [],
      "source": [
        "def encoder_layer(x, Wq, Wk, Wv, Wo):\n",
        "    #Sua resposta começa aqui\n",
        "\n",
        "    #Sua resposta acaba aqui\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgjYs4byyNjc"
      },
      "source": [
        "Agora você pode testar a sua implementação das questões anteriores, executando a célula abaixo. Execute-a sem modificá-la. Nenhum resultado específico é esperado. Apenas que tudo rode sem erros de dimensionalidade gerando um output de formato (4,8)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hx-J6tksyIM",
        "outputId": "d0857c31-6947-46f7-f785-4f28480e1751"
      },
      "outputs": [],
      "source": [
        "#Preenchemos os parâmetros com valores aleatórios para fazer um teste de\n",
        "#sanidade com  a arquitetura implementada\n",
        "embedding_length = 8\n",
        "sequence_length = 4\n",
        "model_dimension = 3\n",
        "input_data = np.random.randn(sequence_length, embedding_length)\n",
        "Wq = np.random.randn(embedding_length, model_dimension)\n",
        "Wk = np.random.randn(embedding_length, model_dimension)\n",
        "Wv = np.random.randn(embedding_length, model_dimension)\n",
        "Wo = np.random.randn(model_dimension, embedding_length)\n",
        "output = encoder_layer(input_data, Wq, Wk, Wv, Wo)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwMX6fity1Ny"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7XImZGPyiOG"
      },
      "source": [
        "# <font color='blue'>  Questão 3 </font>\n",
        "Para finalizar, descreva de forma concisa quais seriam as diferenças na sua implementação se, ao invés de implementar uma camada de Encoder, você fosse implementar um Decoder do Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StWP0aSWzC4Z"
      },
      "source": [
        "**<font color='red'> Sua resposta aqui </font>**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
