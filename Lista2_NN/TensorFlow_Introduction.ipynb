{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/alan-barzilay/NLPortugues/master/imagens/logo_nlportugues.png\"   width=\"150\" align=\"right\">\n",
    "\n",
    "\n",
    "# Introdução ao TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-rc3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensores\n",
    "\n",
    "Um [tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) é um array retangular n-dimensional (uma matriz com _n_ índices) e é o objeto mais basico do TensorFlow. Ele é usado para representar seus dados e faze-los fluir de operação em operação, dai que surgiu o nome TensorFlow.\n",
    "\n",
    "Nesse notebook você terá uma breve introdução a tensores e como usa-los, para mais detalhes recomendamos olhar o [guia oficial](https://www.tensorflow.org/guide/tensor) da equipe TensorFlow.\n",
    "\n",
    "-----\n",
    "Para aqueles que estão familiarizados com numpy, tensores são muito semelhantes a numpy ndarrays, tanto em funcionalidade quanto em utilização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([2,3,4])  # um vetor com 2 matrizes 3 x 4, zerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones([2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como criar um tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1,2,3])   # esse tensor pode ser visto como um vetor de inteiros de tamanho 3, constante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.array([1,2,3])) # idem, mas o vetor é de long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=uint64, numpy=array([1, 2, 3], dtype=uint64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1,2,3], dtype=np.uint ) #idem, unsigned long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Vs. Variable\n",
    "Existem 2 tipos basicos de tensores, [constantes](https://www.tensorflow.org/api_docs/python/tf/constant) e [variaveis](https://www.tensorflow.org/api_docs/python/tf/Variable):\n",
    " - Tensores constante estão \"escritos em pedra\", seus valores não podem ser mais alterados mas podem ser usados como inputs para funções. \n",
    " - Tensores variaveis podem ter seus valores alterados ao realizarmos operações sobre eles, o modulo `tf.keras` os utiliza internamente para representar os pesos de um modelo. Ao inicializarmos um tensor variavel seu `dtype` e `shape` são fixados. Para mais informações, checar este [guia](https://www.tensorflow.org/guide/variable).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1,2,3,4,5])\n",
    "b = tf.constant([5,4,3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([6 6 6 6 6], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "print(c.numpy()) # converte o tensor para um array de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:4].numpy()  # slicing em tensores funciona da mesma maneira que em listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(5,) dtype=int32, numpy=array([ 1, 19,  3,  4,  5], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = tf.Variable([1,2,3,4,5])\n",
    "f[1].assign(19) # tensores do tipo variaveis podem ter seus conteudos alterados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2e571ca07faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tensores do tipo constante não\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "a[1].assign(17) # tensores do tipo constante não"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Além dessa dicotomia básica também temos 2 outros tipos especiais de tensores:\n",
    " - [Ragged Tensors](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) - São basicamente tensores não retangulares, onde nem todas as dimensoes dos elementos são iguais.\n",
    " \n",
    " ![alt-text](https://www.tensorflow.org/guide/images/tensor/ragged.png \"Ilustração RaggedTensor\")\n",
    " \n",
    " - [Sparce Tensors](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor) - Tensores onde a maioria dos seus elementos são nulos.\n",
    " ![alt-text](https://www.tensorflow.org/guide/images/tensor/sparse.png \"Ilustração SparceTensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Como comentado anteriormente, o TensorFlow segue a API do keras. Utilizando as camadas ja existentes no Keras podemos facilmente construir um modelo ao ligarmos elas de maneira sequencial. Uma vez que o modelo esteja definido, basta compila-lo e então treina-lo.\n",
    "A seguir temos um exemplo minimal de uma rede neural *feedforward*. Para mais informações recomendamos conferir a [documentação oficial](https://keras.io/guides/sequential_model/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 16        \n",
      "=================================================================\n",
      "Total params: 35\n",
      "Trainable params: 35\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [   keras.Input(shape=(4,)),\n",
    "        layers.Dense(2, activation=\"relu\"),\n",
    "        layers.Dense(3, activation=\"relu\"),\n",
    "        layers.Dense(4, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma maneira alternativa de criarmos um modelo é seguindo a API funcional do Keras, mas como neste momento estamos lidando apenas com modelos simples e sequenciais isso não é necessario. Para mais informações recomendamos conferir a [documentação oficial](https://keras.io/guides/functional_api/)\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "__Pergunta:__ Em relação à rede definida, qual é o tamanho da entrada, qual o tamanho da saída, quantas camadas internas e quais são os seus tamanhos? Você seria capaz de desenhá-la?\n",
    "\n",
    "------------------------------------------------------\n",
    "Tendo definido nosso modelo, precisamos escolher um *optimizer*, uma função *loss* e compilar ele.\n",
    "Para mais informações sobre diferentes otimizadores, recomendamos [este post](https://ruder.io/optimizing-gradient-descent/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.compile(optimizer= 'rmsprop', loss='binary_crossentropy', metrics = None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim só nos resta treinar nosso modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "model.fit(x= dados_treino, y= labels_treino, batch_size=32, epochs=300)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________\n",
    "\n",
    "## TF 1.X  Vs TF 2.0\n",
    "\n",
    "Aqui vamos explorar brevemente alguma das principais diferenças e novidades introduzidas no começo de 2019 com TF 2.0. O contexto aqui apresentado poderá ser util mesmo para alunos que nunca tiveram contato com TF 1.X.\n",
    "\n",
    "### Eager execution\n",
    "\n",
    "Em TF 1.X o exemplo que utilizamos acima com `c = a + b` não retornaria o resultado da soma desses 2 tensores, a avaliação das variaveis era adiada até o usuario incializar um objeto session, inicializar as variaveis e em seguida executar cada uma das operações com o método `.run()`.\n",
    "\n",
    "Isso era feito desta maneira para possibilitar a geração de um grafo onde seus vértices representavam operações e suas arestas tensores fluindo de uma operação a outra. Esse grafo possuia varias utilidades, permitindo uma maior portabilidade dos modelos e maior eficiencia. Porém tornava muito mais dificil de debuggar e encontrar erros no código além de acrescentar *boilerplate code*.\n",
    "\n",
    "Com a mudança para o TF 2.0 tudo isso mudou visando simplificar a biblioteca, com o objetivo de tornar o codigo mais \"pythonico\" e semelhante ao resto da linguagem. `Sessions` foram abolidas e eager execution foi ligado por default. Ou seja, ao somarmos 2 tensores essa operação será realizada automaticamente como qualquer operação em python.\n",
    "\n",
    "Apesar de simplificar sensivelmente a sintaxe, a eficiência do código acabou sendo afetada uma vez que não estamos utilizando os grafos. Para poder disfrutarmos da simplicidade da *eager execution* e da eficiencia do *graphmode*, TF 2.0 introduziu o decorador  `tf.function` e o modo autograph que exploraremos mais a fundo em outro momento. A perda de performance não é grande o suficiente para afetar as analises simples que iremos realizar no começo dessa matéria e, portanto, optamos por deixar os alunos se familiarizarem com o funcionamento do TF 2.0 antes de nos preocuparmos em como otimizar esse código. Além disso, nós só precisamos nos preocupar com  usar `tf.function` em funções que nós mesmos definirmos, de maneira geral todas as funções fornecidas pelo tensorflow já cuidam disso por nós.\n",
    "Os alunos ansiosos podem conferir o [guia da equipe TensorFlow](https://www.tensorflow.org/guide/function).\n",
    "\n",
    "\n",
    "### Keras API\n",
    "\n",
    "A API do Keras `tf.keras` foi instituida como a maneira padrão de se escrever e desenvolver redes neurais em TF2.0. No TensorFlow 1.X isso nem sempre era verdade e podiamos gerar camadas de diversas maneiras distintas. \n",
    "Com essa adoção também foi possivel remover diversos métodos duplicados e simplificar consideravelmente a biblioteca.\n",
    "\n",
    "### `tf.contrib`\n",
    "Esse modulo foi completamente removido e seus componentes foram redistribuidos e agregados em outros modulos, se você encontrar algum guia ou função que utilize `tf.contrib` saiba que ele se refere ao TF 1.X e pode estar desatualizado.\n",
    "\n",
    "\n",
    "\n",
    "_______________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
