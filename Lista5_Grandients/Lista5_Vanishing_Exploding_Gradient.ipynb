{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCi_bd9wr96"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/alan-barzilay/NLPortugues/master/imagens/logo_nlportugues.png\"   width=\"150\" align=\"right\">\n",
        "\n",
        "\n",
        "# Lista 5 - Vanishing & Exploding Gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "______________\n"
      ],
      "metadata": {
        "id": "roQtHd9Sw2da"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXMOjXMowr98"
      },
      "source": [
        "\n",
        "Nessa lista exploraremos alguns problemas que podemos encontrar ao treinarmos uma rede recorrente. Esses problemas não são únicos das redes recorrentes, qualquer rede profunda pode sofrer de vanishing e exploding gradient, mas as redes recorrentes são especialmente instaveis devido a utilização da mesma matriz de pesos repetidas vezes.\n",
        "\n",
        "Começaremos explorando o exploding gradient e alguns de seus sintomas, em seguida utilizaremos gradient cliping para combate-lo.\n",
        "Por fim, estudaremos uma rede que sofre de vanishing gradient e tambem veremos uma estratégia para evitá-lo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSRCvXy9wr99"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from numpy import where\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.initializers import RandomUniform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnZHW-YLwr9-"
      },
      "source": [
        "\n",
        "\n",
        "# Exploding Gradient\n",
        "\n",
        "Para essa parte da lista nós preparamos uma rede simples, note como a *loss* cresce exponencialmente até virar infinita e logo em seguida NaN. Esse é um sintoma classico de exploding gradient. O gradiente está tão elevado que a cada etapa de backpropagation o passo de atualização dos parametros leva a um aumento na *loss* e isso segue crescendo até que exploda.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXt0hAhUwr9_"
      },
      "outputs": [],
      "source": [
        "def f1(x):\n",
        "    # função afim que pediremos para a rede aproximar\n",
        "    return 5+ 10*x\n",
        "\n",
        "xs = [x for x in range(100)]\n",
        "ys = [f1(x) for x in range(100)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkm6HLjywr9_"
      },
      "outputs": [],
      "source": [
        "opt = keras.optimizers.SGD()\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "model.compile(optimizer=opt, loss=\"mean_squared_error\")\n",
        "model.fit(xs,ys,epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmslLcmcwr-A"
      },
      "source": [
        "## Gradient Cliping\n",
        "## <font color='blue'>Questão 1 </font>\n",
        "\n",
        "\n",
        "Exploraremos agora a técnica do *Gradient Cliping* para evitar o problema da explosão do gradiente. Exploraremos duas variações distintas dessa estratégia. A primeira é o `clipvalue`. Nessa abordagem impedimos que os valores das derivadas que compõe o vetor de gradiente saiam de um intervalo determinado. Pesquise na documentação do tensorflow por como adicionar a abordagem de clipvalue ao otimizador e refaça a rede anterior com essa mudança. Tente diferentes valores de intervalo até encontrar um que garanta o comportamento esperado do loss. Compile e treine seu novo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RCs6UXRwr-A"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "\n",
        "#Seu código aqui\n",
        "\n",
        "\n",
        "model.fit(xs,ys,epochs=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='blue'>Questão 2 </font>\n",
        "\n",
        "Agora exporaremos uma estratégia alternativa, o `clipnorm`. Nessa estratégia ao invés de limitarmos as componentes do vetor de gradiente, limitamos a sua norma a um intervalo pré-determinado.\n",
        "\n",
        "De maneira análoga à questão anterior, pesquise na documentação por como aplicar essa estratégia ao otimizador e refaça a rede anterior, compilando-a e treinando-a para evitar o problema da explosão do gradiente.\n",
        "\n"
      ],
      "metadata": {
        "id": "CZ50BIQbCwGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "\n",
        "#Seu código aqui\n",
        "\n",
        "\n",
        "model.fit(xs,ys,epochs=400)"
      ],
      "metadata": {
        "id": "z_GLNLIqDiQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLFY7mXWwr-B"
      },
      "source": [
        "________________________\n",
        "\n",
        "\n",
        "\n",
        "# Vanishing\n",
        "\n",
        "Lidar com Vanishing Gradient é muito mais desafiador do que com exploding gradient. Não é trivial determinar se o baixo desempenho de sua rede é causado por vanishing gradient, uma vez que seus sintomas são relativamente genéricos e ele pode ser apenas mais um dos fatores que prejudicam seu desempenho. Além disso não existe uma solução geral e definitiva, como o gradient cliping em casos de explosão do gradiente.\n",
        "\n",
        "Preparamos algumas redes para poder explorar um caso mais simples de vanishing gradient e também uma possivel solução. Começamos gerando um dataset simples de classificação e treinamos uma rede rasa que obtem uma boa performance.\n",
        "Ao aprofundarmos essa rede podemos notar que sua performance cai drasticamente se tornando quase tão eficiente quanto jogar uma moeda para chutar a classe do ponto, ela nem mesmo é capaz de \"*overfittar*\" os dados. Utilizaremos uma nova forma de inicialização dos pesos da rede para tentar recuperar nossa performance e novamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc7EGPRFwr-C"
      },
      "source": [
        "## Definindo nossos dados\n",
        "Primeiro definiremos um toy dataset bem simples que utilizaremos para nossos modelos e uma função auxiliar para facilitar a comparação de nossos modelos.\n",
        "\n",
        "\n",
        "Esses dados e redes foram inspirados e adaptados [deste post](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0uijES3wr-E"
      },
      "outputs": [],
      "source": [
        "# gera dataset de classificação\n",
        "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
        "\n",
        "# escala input para [-1,1]\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# plota visualização do dataset\n",
        "for i in range(2):\n",
        "    samples_ix = where(y == i)\n",
        "    plt.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# separa em teste e treino\n",
        "n_train = 500\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csZjcqGfwr-E"
      },
      "outputs": [],
      "source": [
        "def run_model(model,log_to_tb= False ,trainX=trainX,trainy=trainy,testX=testX,testy=testy):\n",
        "    \"\"\"\n",
        "    Função auxiliar que recebe um modelo e realiza seu treinamento e avaliação no dataset.\n",
        "    \"\"\"\n",
        "    model.summary()\n",
        "\n",
        "    # compila modelo\n",
        "    opt = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    # fit modelo\n",
        "    history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=0)\n",
        "\n",
        "    # avalia modelo\n",
        "    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "    _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "    print(\"\\n\")\n",
        "    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "\n",
        "    # plota acurácia/training history\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"Acurácia \"+ model.name)\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='test')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XiWOdItwr-F"
      },
      "source": [
        "## Rede rasa\n",
        "Aqui temos uma rede rasa com apenas uma camada oculta e uma de output, note que ela é capaz de atingir uma performance razoavel após 300 epochs.\n",
        "\n",
        "Nós estamos utilizando um inicializador diferente do padrão para os pesos da camada, que sorteia os pesos iniciais a partir de uma distribuição uniforme no intervalo [0,1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4XQT0w8wr-F"
      },
      "outputs": [],
      "source": [
        "#define modelo raso\n",
        "init = RandomUniform(minval=0, maxval=1)\n",
        "\n",
        "model = keras.Sequential(name=\"modelo_raso\")\n",
        "model.add(keras.layers.Dense(5,\n",
        "                       input_dim=2,\n",
        "                       activation=\"tanh\",\n",
        "                       kernel_initializer=init,\n",
        "                       name=\"raso_1\"))\n",
        "model.add(keras.layers.Dense(1,\n",
        "                       activation='sigmoid',\n",
        "                       kernel_initializer=init,\n",
        "                       name=\"raso_output\"))\n",
        "\n",
        "\n",
        "run_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wJ1Y-zFwr-G"
      },
      "source": [
        "## Rede funda\n",
        "\n",
        "Agora tornaremos nossa rede mais funda com 5 camadas ocultas e uma de output, note como a performance cai drasticamente e se torna próxima a um chute aleatório. Embora o modelo seja mais complexo e poderoso nós não conseguimos treina-lo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz1ncqe6wr-G"
      },
      "outputs": [],
      "source": [
        "# define modelo mais fundo\n",
        "init = RandomUniform(minval=0, maxval=1)\n",
        "\n",
        "model = Sequential(name=\"modelo_fundo\")\n",
        "model.add(Dense(5, input_dim=2, activation='tanh', kernel_initializer=init,name=\"funda_1\"))\n",
        "model.add(Dense(5, activation='tanh', kernel_initializer=init,name=\"funda_2\"))\n",
        "model.add(Dense(5, activation='tanh', kernel_initializer=init,name=\"funda_3\"))\n",
        "model.add(Dense(5, activation='tanh', kernel_initializer=init,name=\"funda_4\"))\n",
        "model.add(Dense(5, activation='tanh', kernel_initializer=init,name=\"funda_5\"))\n",
        "model.add(Dense(1, activation='sigmoid', kernel_initializer=init,name=\"funda_output\"))\n",
        "\n",
        "\n",
        "run_model(model,log_to_tb=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX9jJMCJwr-G"
      },
      "source": [
        "### Utilizando inicialização de Xavier Glorot\n",
        "\n",
        "Agora utilizaremos uma técnica de combate ao vanishing gradient, utilizaremos outro inicializador para os pesos da rede. O [inicializador de Xavier Glorot](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) é o inicializador default de algumas camadas do keras como a camada densa que utilizamos.\n",
        "\n",
        "## <font color='blue'>Questão 3 </font>\n",
        "Repita, na célula seguinte a rede da célula anterior, mas trocando o seu incializador pelo de Xavier Glorot uniforme, de maneira explícita. Treine o novo modelo usando a função `run model`. Faça uma análise comparativa (com o mesmo tamanho de um tweet) das curvas de obtidas para esta rede e para a rede anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4FV54uewr-G"
      },
      "outputs": [],
      "source": [
        "#Seu código aqui\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='red'> Sua análise aqui </font>"
      ],
      "metadata": {
        "id": "CHz8dWJPSvpY"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}