{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALcA0Obdu4gt"
      },
      "source": [
        "# Lista 6 - Métricas de Desemepnho\n",
        "\n",
        "Nessa lista exploraremos algumas das métricas de performance mais comuns em problemas de processamento de linguagem natural para tentar entendê-las mais a fundo. Para facilitar, nos restringiremos a métricas de classificação binária, mas esperamos que muito da discussão aqui apresentada possa ser generalizada para outras famílias de métricas.\n",
        "\n",
        "Começamos importando as métricas da classificação já pré-implementadas da biblioteca [scikit-learn](https://scikit-learn.org/stable/), uma das principais bibliotecas de aprendizado de máquina. A ideia dessa lista é implementar essas métricas na mão para melhor entendê-las, mas usaremos a implementação do scikit-learn para fins de comparação, garantindo a corretude das nossas implementações.\n",
        "\n",
        "Quando lidamos com problemas reais, o recomendado é sempre utilizar as métricas pré-implementadas, diminuindo a chance de erros. Outras boibliotecas também têm suas próprias versões dessas métricas implementadas, que podem ser integradas mais facilmente ao seu fluxo próprio de trabalho. Já vimos, por exemplo, como computar a acurácia de uma rede neural treinada no keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj0WsIA9XLLx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score as accuracy\n",
        "from sklearn.metrics import precision_score as precision\n",
        "from sklearn.metrics import recall_score as recall\n",
        "from sklearn.metrics import f1_score as f1\n",
        "SEED = 18272\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19070YncwolQ"
      },
      "source": [
        "## Função de testes\n",
        "\n",
        "Como primeiro passo, definimos uma função de testes, cujo intuito é permitir uma comparação consistente das métricas que você implementará  ao longo da lista com as implementações do scikit-learn.\n",
        "\n",
        "Como dito anteriormente,  nos restringiremos, nessa lista, a problemas de classificação binária e essa função de testes pressupṍe isso.\n",
        "\n",
        "O problema de classificção binária é o problema de classificar cada um dos objetos de um conjunto de interesse em uma de duas classes. Usualment, queremos desenvolver/treinar um classificador que aproxime bem a real distribuição das classes do problema.\n",
        "\n",
        "É nesse \"aproximar bem\" que mora o desafio. A não ser para problemas triviais, nunca conseguiremos obter um classificador que emule perfeitamente a distribuição real dos dados, então precisamos nos contentar com aproximações. Entretanto, quando falamos de uma distribuição de probabilidades, aproximações possíveis podem varias sob diversos aspectos e nós precisamos escolher, para o nosso problema, quais aspectos são relevantes e devem ser priorizados durante o processo de aproximação.\n",
        "\n",
        "Quando escolhemos um conjunto de métricas de desempenho para avaliar os nossos modelos, estamos escolhendo também um conjunto de aspectos que estamos considerando relevantes para definir o que é uma boa aproximação, dada a nossa aplicação.\n",
        "\n",
        "s métricas de desempenho podem variar muito entre si e temos uma grande quantidade delas a nossa disposição, cada uma com suas peculiaridades, cada uma privilegiando determinados aspectos das distribuições de probabilidade que estamos tentando aproximar. No geral, entretanto, métricas de desempenho são computadas em função do conjunto de previsões retornadas por um modelo e o conjunto de respostas verdadeiras que gostaríamos que a quele modelo retornasse (chamadas de padrão-ouro), representados, respectivamente, por `y_pred` e por `y_gold` no código abaixo.\n",
        "\n",
        "Para testar as métricas sem depender de nenhum modelo em específico, na função abaixo produzimos conjuntos aletórios de previsões e de padrões-ouro e comparamos sob cada um deles uma métrica programada pelo usuário e uma referência. Como estamos trabalhando com classificações binárias, os vetores produzidos para o teste são preenchidos apenas por 0s e por 1s, representando cada uma das possíveis classes, usualmente chamadas de classe negativa e positiva, respectivamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sB_rbQ1XMHa"
      },
      "outputs": [],
      "source": [
        "def test_metric(user_metric, reference_metric, num_tests=100, samples_per_test=100, seed=SEED, tolerance=1e-4):\n",
        "    np.random.seed(SEED)\n",
        "    for i in tqdm(range(num_tests)):\n",
        "        y_pred = list(np.random.randint(2, size=samples_per_test))\n",
        "        y_gold = list(np.random.randint(2, size=samples_per_test))\n",
        "        user_metric_value = user_metric(y_gold, y_pred)\n",
        "        reference_metric_value = reference_metric(y_gold, y_pred)\n",
        "        if abs(user_metric_value - reference_metric_value) > tolerance:\n",
        "            raise ValueError(f\"Test Failed. user_metric returned {user_metric_value}, but {reference_metric_value} was expected.\")\n",
        "    print(\"\\nThe function passed all tests\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCMlYvHXzo-t"
      },
      "source": [
        "# Acurácia\n",
        "\n",
        "A métrica mais utilizada na literatura é a acurácia, que já vimos várias vezes ao longo do curso. Ela representa o número de previsões que coincidem com a resposta verdadeira e nos fornece uma visão geral da qualidade do modelo.\n",
        "\n",
        "# <font color='blue'>  Questão 1 </font>\n",
        "\n",
        "Complete a função `my_accuracy` na célula abaixo, de forma que ela compute corretamente a acurácia de `y_pred` dado `y_gold`. Em seguida, rode a função de testes para testar se a sua implementação está compatível com a da biblioteca scikit-learn. Você pode usar a definição de acurácia para problemas de classificação disponível [aqui](https://en.wikipedia.org/wiki/Accuracy).\n",
        "Não esqueça de preparar a sua função para lidar com o caso em que o denominador da acurácia é nulo. (Como sabemos, divisões por zero geralmente causam pânico moral)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZnqc_cZUdex"
      },
      "outputs": [],
      "source": [
        "def my_accuracy(y_gold, y_pred):\n",
        "    \n",
        "    # Seu código aqui\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNGgYmyBayuH",
        "outputId": "99ee898d-95fc-4e78-ce29-aaa4a4708c62"
      },
      "outputs": [],
      "source": [
        "test_metric(my_accuracy, accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54blpCnc2Nyu"
      },
      "source": [
        "# Precisão e Cobertura\n",
        "\n",
        "O problema da acurácia é que, apesar de fornecer um índice geral e fácil comparabilidade para a performance de diferentes modelos, ela captura poucas nuances da performance desses modelos. Por esse motivo, outras métricas largamente utilizadas, que permitem capturar algumas dessas nuances, são a Precisão e a Cobertura (chamada em inglês de recall).\n",
        "\n",
        "Definições formais da precisão e da cobertura podem ser encontradas [aqui](https://en.wikipedia.org/wiki/Precision_and_recall), entretanto, colocando de uma forma pseudo-intuitiva, precisão e cobertura podem ser definidias como as respostas para as seguintes perguntas:\n",
        "    - Pecisão: Daquilo que eu previ como positivo, quanto de fato era positivo?\n",
        "    - Cobertura: Daquilo que eu deveria ter previsto como positivo, quanto de fato eu previ?\n",
        "\n",
        "É importante notar que, sim, previsão e cobertura são complementares em diversos sentidos. Na próxima seção discutiremos um pouco sobre isso.\n",
        "\n",
        "# <font color='blue'>  Questão 2 </font>\n",
        "\n",
        "Preencha as funções `my_precision` e `my_recall`, nas células seguintes, de forma a computar a precisão e a cobertura de `y_pred` com respeito a `y_gold`. Em seguida, rode as células de teste para garantir que suas implementações são compatíveis com as do scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDLSPF32bH9q"
      },
      "outputs": [],
      "source": [
        "def my_precision(y_gold, y_pred):\n",
        "    \n",
        "    # Seu código aqui\n",
        "    \n",
        "    return precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yQVEjyUcAS9",
        "outputId": "1c2ba945-bb43-4102-d722-e827e6eabc62"
      },
      "outputs": [],
      "source": [
        "test_metric(precision, my_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W0nJophmkRu"
      },
      "outputs": [],
      "source": [
        "def my_recall(y_gold, y_pred):\n",
        "\n",
        "        # Seu código aqui\n",
        "        \n",
        "        return recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw8OYlCim_ns",
        "outputId": "a60b4ddc-a88e-41af-e831-ca48071b3320"
      },
      "outputs": [],
      "source": [
        "test_metric(my_recall, recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIkZdMRS4Hha"
      },
      "source": [
        "# Medidas $F_\\beta$\n",
        "\n",
        "Como dito anteriormente, Precisão e Cobertura são complementares. Existe uma relação de compensasão entre elas, que implica uma tendência da cobertura cair quando a precisão aumenta e vice-versa. Por esse motivo, muitas vezes computamos também uma medida única que combina precisão e cobertura, chamada de Medida-$F_\\beta$, ou simplesmente Medida-F. O parâmetro $\\beta$ controla o balanço entre a precisão e acurácia, permitindo que você privilegie uma delas sobre a outra na hora de combiná-las. Normalmente, isso é dado pelas necessidades específicas do problema que se está querendo resolver. A forma com $\\beta=1$ dá igual importânica para precisão e cobertura e é a mais utilizada.\n",
        "\n",
        "# <font color='blue'>  Questão 3 </font>\n",
        "\n",
        "Complete a função `my_f1`, a seguir de forma a computar a Medida $F_1$ (Medida $F_\\beta$ com $\\beta=1$) entre `y_pred` e `y_gold`. Em seguida, execute a próxima célula de maneira a testar a sua implementação. Por fim, reflita um pouco sobre o equilíbrio entre precisão e cobertura e escreva no local indicado em vermelho uma situação ou problema de classificação binário em que faria sentido privilegiar a precisão e um outro em que faria sentido privilegiar a cobertura.\n",
        "\n",
        "Observações: [Aqui](https://en.wikipedia.org/wiki/F1_score), você encontra uma definição formal da Medida F1. Você pode usar as suas implementações de precisão e cobertura, feitas anteriormente, para calcular a medida F1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t99wzZuWngsw"
      },
      "outputs": [],
      "source": [
        "def my_f1(y_gold, y_pred):\n",
        "        \n",
        "        # Seu código aqui\n",
        "\n",
        "        return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mb3wuLXn80X",
        "outputId": "51c5449d-4874-4d5a-a7b7-cdfedea54003"
      },
      "outputs": [],
      "source": [
        "test_metric(my_f1, f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ-PtkWf6pHZ"
      },
      "source": [
        "**<font color='red'> Insira aqui sua resposta </font>**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
